"""
ì˜ë¯¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ëª¨ë“ˆ
- ë‹¤êµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ (spaCy)
- ë¬¸ì„œ êµ¬ì¡° ì¸ì‹ (ì œëª©, í‘œ, ë¦¬ìŠ¤íŠ¸, ì„¹ì…˜)
- ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë¬¸ì„œëª…, ë‚ ì§œ, ë¶€ì„œ, ìŠ¹ì¸ì)
- ì˜ë¯¸ ë‹¨ìœ„ ë³‘í•© (ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ì•ˆ ìë¦„)
"""

from typing import List, Dict, Optional, Tuple
import re
from datetime import datetime

# ì–¸ì–´ ê°ì§€
try:
    from langdetect import detect, LangDetectException
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False

# spaCy (ë‹¤êµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬)
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False


class SemanticTextSplitter:
    """ì˜ë¯¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""

    def __init__(self, config):
        self.config = config
        self.chunk_size = config.chunk_size
        self.chunk_overlap = config.chunk_overlap

        # spaCy ëª¨ë¸ ë¡œë“œ
        self.nlp_models = {}
        if SPACY_AVAILABLE:
            try:
                self.nlp_models['ko'] = spacy.load("ko_core_news_sm")
                print("  âœ“ spaCy í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                print("  âš ï¸ spaCy í•œêµ­ì–´ ëª¨ë¸ ë¯¸ì„¤ì¹˜")

            try:
                self.nlp_models['en'] = spacy.load("en_core_web_sm")
                print("  âœ“ spaCy ì˜ì–´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                print("  âš ï¸ spaCy ì˜ì–´ ëª¨ë¸ ë¯¸ì„¤ì¹˜")

        if not self.nlp_models:
            print("  âš ï¸ spaCy ëª¨ë¸ì´ ì—†ì–´ ê¸°ë³¸ ë¶„í•  ì‚¬ìš©")

    def detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€ (í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ë“±)"""
        if not LANGDETECT_AVAILABLE or not text.strip():
            return 'ko'  # ê¸°ë³¸ê°’: í•œêµ­ì–´

        try:
            lang = detect(text)
            # ko, en, ja ë“±
            return lang if lang in ['ko', 'en', 'ja'] else 'ko'
        except LangDetectException:
            return 'ko'

    def extract_metadata(self, text: str) -> Dict:
        """ë¬¸ì„œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}

        # ë¬¸ì„œ ì œëª© ì¶”ì¶œ (ì²« ì¤„ì´ ì§§ê³  ë…ë¦½ì ì¸ ê²½ìš°)
        lines = text.split('\n')
        if lines and len(lines[0].strip()) < 100:
            potential_title = lines[0].strip()
            if potential_title and not potential_title.endswith(('.', 'ã€‚', '?', '!')):
                metadata['title'] = potential_title

        # ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ
        date_patterns = [
            r'\d{4}[-ë…„./]\s*\d{1,2}[-ì›”./]\s*\d{1,2}',  # 2025-01-15, 2025ë…„ 1ì›” 15ì¼
            r'\d{4}\.\s*\d{1,2}\.\s*\d{1,2}',  # 2025. 1. 15
            r'\d{4}/\d{1,2}/\d{1,2}',  # 2025/01/15
        ]

        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                metadata['date'] = match.group(0)
                break

        # ë¶€ì„œ/íŒ€ ì¶”ì¶œ
        dept_patterns = [
            r'(ì†Œì†|ë¶€ì„œ|íŒ€)\s*[:ï¼š]?\s*([ê°€-í£]+(?:íŒ€|ë¶€|ê³¼|ì„¼í„°))',
            r'([ê°€-í£]+(?:íŒ€|ë¶€|ê³¼|ì„¼í„°))',
        ]

        for pattern in dept_patterns:
            match = re.search(pattern, text)
            if match:
                metadata['department'] = match.group(2) if match.lastindex >= 2 else match.group(1)
                break

        # ì‘ì„±ì ì¶”ì¶œ
        author_patterns = [
            r'(ì‘ì„±ì|ê¸°ì•ˆì|ë‹´ë‹¹ì)\s*[:ï¼š]?\s*([ê°€-í£]{2,4})\s*(íŒ€ì›|ëŒ€ë¦¬|ê³¼ì¥|ì°¨ì¥|ë¶€ì¥|ì´ì‚¬)?',
        ]

        for pattern in author_patterns:
            match = re.search(pattern, text)
            if match:
                name = match.group(2)
                position = match.group(3) if match.lastindex >= 3 else ''
                metadata['author'] = f"{name} {position}".strip()
                break

        # ë¬¸ì„œ íƒ€ì… ê°ì§€
        doc_type_keywords = {
            'íšŒì˜ë¡': ['íšŒì˜ë¡', 'ë¯¸íŒ…', 'íšŒì˜'],
            'ìš”ì²­ì„œ': ['ìš”ì²­ì„œ', 'ì‹ ì²­ì„œ', 'ì˜ë¢°ì„œ'],
            'ë³´ê³ ì„œ': ['ë³´ê³ ì„œ', 'ê²°ê³¼ ë³´ê³ ', 'ì§„í–‰ ë³´ê³ '],
            'ê³„íšì„œ': ['ê³„íšì„œ', 'ê¸°íšì„œ', 'ì œì•ˆì„œ'],
            'ìŠ¹ì¸ë¬¸ì„œ': ['ìŠ¹ì¸', 'ê²°ì¬', 'ê¸°ì•ˆ'],
        }

        for doc_type, keywords in doc_type_keywords.items():
            if any(keyword in text[:200] for keyword in keywords):
                metadata['doc_type'] = doc_type
                break

        return metadata

    def detect_structure(self, text: str) -> List[Dict]:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (ì œëª©, í‘œ, ë¦¬ìŠ¤íŠ¸, ì„¹ì…˜)"""
        structures = []
        lines = text.split('\n')

        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue

            # ì œëª© ê°ì§€ (ì§§ê³  ë²ˆí˜¸ê°€ ìˆê±°ë‚˜ ë…ë¦½ì )
            if len(line) < 100:
                # "1. ì œëª©", "ê°€. ì œëª©", "[ì œëª©]" íŒ¨í„´
                if re.match(r'^[\dê°€-í£]+[\.\)]\s*[ê°€-í£]', line) or re.match(r'^\[.+\]$', line):
                    structures.append({
                        'type': 'heading',
                        'line': i,
                        'text': line
                    })

            # í‘œ ê°ì§€ (íƒ­ì´ë‚˜ íŒŒì´í”„ë¡œ êµ¬ë¶„)
            if '\t' in line or '|' in line or 'â”‚' in line:
                structures.append({
                    'type': 'table',
                    'line': i,
                    'text': line
                })

            # ë¦¬ìŠ¤íŠ¸ ê°ì§€ (-, *, ë²ˆí˜¸)
            if re.match(r'^[\s]*[-\*â€¢]\s', line) or re.match(r'^[\s]*\d+[\.\)]\s', line):
                structures.append({
                    'type': 'list',
                    'line': i,
                    'text': line
                })

        return structures

    def split_sentences(self, text: str, language: str = 'ko') -> List[str]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (spaCy ì‚¬ìš©)"""
        if language in self.nlp_models:
            nlp = self.nlp_models[language]
            doc = nlp(text)
            sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
            return sentences
        else:
            # spaCy ì—†ì„ ë•Œ í´ë°±: ê°„ë‹¨í•œ ì •ê·œì‹ ë¶„ë¦¬
            return self._fallback_sentence_split(text)

    def _fallback_sentence_split(self, text: str) -> List[str]:
        """spaCy ì—†ì„ ë•Œ ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„ë¦¬"""
        # í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ ì¢…ê²° ê¸°í˜¸ë¡œ ë¶„ë¦¬
        sentences = re.split(r'([.!?ã€‚ï¼ï¼Ÿ]+[\s\n]+)', text)

        # ë¬¸ì¥ + êµ¬ë¶„ì í•©ì¹˜ê¸°
        result = []
        for i in range(0, len(sentences)-1, 2):
            sent = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')
            if sent.strip():
                result.append(sent.strip())

        # ë§ˆì§€ë§‰ ë¬¸ì¥
        if len(sentences) % 2 == 1 and sentences[-1].strip():
            result.append(sentences[-1].strip())

        return result

    def merge_chunks_semantically(
        self,
        sentences: List[str],
        structures: List[Dict],
        max_chunk_size: int = None
    ) -> List[str]:
        """ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì¥ë“¤ì„ ì²­í¬ë¡œ ë³‘í•©"""
        if max_chunk_size is None:
            max_chunk_size = self.chunk_size

        chunks = []
        current_chunk = []
        current_size = 0

        for sentence in sentences:
            sent_size = len(sentence)

            # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ì‹œì‘
            if current_size + sent_size > max_chunk_size and current_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunks.append(' '.join(current_chunk))

                # ì˜¤ë²„ë©: ë§ˆì§€ë§‰ 1~2 ë¬¸ì¥ ìœ ì§€
                overlap_size = 0
                overlap_sents = []
                for sent in reversed(current_chunk):
                    if overlap_size + len(sent) <= self.chunk_overlap:
                        overlap_sents.insert(0, sent)
                        overlap_size += len(sent)
                    else:
                        break

                current_chunk = overlap_sents
                current_size = overlap_size

            current_chunk.append(sentence)
            current_size += sent_size

        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks

    def split(self, pages_data: List[Dict]) -> List[Dict]:
        """í˜ì´ì§€ ë°ì´í„°ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ ì²­í¬ë¡œ ë¶„í• """
        all_chunks = []
        chunk_id = 0

        print(f"\nğŸ”„ SemanticTextSplitter ì‹œì‘")
        print(f"  - ì²­í¬ í¬ê¸°: {self.chunk_size}ì")
        print(f"  - ì˜¤ë²„ë©: {self.chunk_overlap}ì")

        for page in pages_data:
            page_num = page.get('page_num', 1)
            text = page.get('text', '')

            if not text or not text.strip():
                all_chunks.append({
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': '',
                    'char_count': 0,
                    'split_method': 'semantic',
                    'warning': 'no_text'
                })
                chunk_id += 1
                continue

            # 1. ì–¸ì–´ ê°ì§€
            language = self.detect_language(text)

            # 2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self.extract_metadata(text)

            # 3. ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
            structures = self.detect_structure(text)

            # 4. ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
            sentences = self.split_sentences(text, language)

            # 5. ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í¬ ë³‘í•©
            chunk_texts = self.merge_chunks_semantically(sentences, structures)

            # 6. ì²­í¬ ìƒì„±
            for chunk_text in chunk_texts:
                chunk_data = {
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': chunk_text,
                    'char_count': len(chunk_text),
                    'split_method': 'semantic',
                    'language': language,
                }

                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                if metadata:
                    chunk_data['metadata'] = metadata

                # êµ¬ì¡° ì •ë³´ ì¶”ê°€
                if structures:
                    chunk_data['has_structure'] = True
                    chunk_data['structure_types'] = list(set(s['type'] for s in structures))

                all_chunks.append(chunk_data)
                chunk_id += 1

        print(f"  âœ“ ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")

        return all_chunks


class HybridSemanticSplitter:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„í• ê¸°: Semantic ìš°ì„ , LangChain í´ë°±"""

    def __init__(self, config):
        self.config = config
        self.semantic_splitter = None
        self.langchain_splitter = None

        # Semantic Splitter ì‹œë„
        if SPACY_AVAILABLE and LANGDETECT_AVAILABLE:
            try:
                self.semantic_splitter = SemanticTextSplitter(config)
                print("âœ“ SemanticTextSplitter í™œì„±í™” (ì˜ë¯¸ ê¸°ë°˜ ë¶„í• )")
            except Exception as e:
                print(f"âš ï¸ SemanticTextSplitter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # LangChain í´ë°±
        if not self.semantic_splitter:
            try:
                from langchain.text_splitter import RecursiveCharacterTextSplitter
                self.langchain_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=config.chunk_size,
                    chunk_overlap=config.chunk_overlap,
                    separators=["\n\n", "\n", "ã€‚", ". ", "! ", "? ", ", ", " ", ""],
                )
                print("âœ“ LangChain ë¶„í•  ëª¨ë“œ (í´ë°±)")
            except ImportError:
                print("âš ï¸ LangChain ë¯¸ì„¤ì¹˜, ê¸°ë³¸ ë¶„í•  ì‚¬ìš©")

    def split(self, pages_data: List[Dict]) -> List[Dict]:
        """ë¶„í•  ì‹¤í–‰"""
        if self.semantic_splitter:
            return self.semantic_splitter.split(pages_data)
        elif self.langchain_splitter:
            return self._split_langchain(pages_data)
        else:
            return self._split_basic(pages_data)

    def _split_langchain(self, pages_data: List[Dict]) -> List[Dict]:
        """LangChain ë¶„í• """
        chunks = []
        chunk_id = 0

        for page in pages_data:
            page_num = page['page_num']
            text = str(page['text'])

            if not text.strip():
                chunks.append({
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': '',
                    'char_count': 0,
                    'split_method': 'langchain',
                    'warning': 'no_text'
                })
                chunk_id += 1
                continue

            split_texts = self.langchain_splitter.split_text(text)

            for split_text in split_texts:
                chunks.append({
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': split_text,
                    'char_count': len(split_text),
                    'split_method': 'langchain',
                })
                chunk_id += 1

        return chunks

    def _split_basic(self, pages_data: List[Dict]) -> List[Dict]:
        """ê¸°ë³¸ ë¶„í• """
        chunks = []
        chunk_id = 0

        for page in pages_data:
            page_num = page['page_num']
            text = str(page['text'])

            if not text.strip():
                chunks.append({
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': '',
                    'char_count': 0,
                    'split_method': 'basic',
                    'warning': 'no_text'
                })
                chunk_id += 1
                continue

            # ë‹¨ìˆœ ê³ ì • ê¸¸ì´ ë¶„í• 
            start = 0
            while start < len(text):
                end = min(start + self.config.chunk_size, len(text))
                chunk_text = text[start:end]

                chunks.append({
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': chunk_text,
                    'char_count': len(chunk_text),
                    'split_method': 'basic',
                })
                chunk_id += 1

                if end >= len(text):
                    break

                start = end - self.config.chunk_overlap

        return chunks
