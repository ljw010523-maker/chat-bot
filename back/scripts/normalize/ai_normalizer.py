"""
T5 ê¸°ë°˜ í•œêµ­ì–´ ë§ì¶¤ë²• êµì • ëª¨ë“ˆ (ì•ˆì „ í”„ë¡¬í”„íŠ¸ + ë§ˆìŠ¤í¬ ë³´í˜¸ + í† í° ë¶„í• )
back/scripts/normalize/ai_normalizer.py
"""

from typing import Dict, List
import time
import re
import contextlib

# transformers ê°€ìš©ì„± í”Œë˜ê·¸ ì´ë¦„ ë¶„ë¦¬ (ë‹¤ë¥¸ ëª¨ë“ˆê³¼ ì¶©ëŒ ë°©ì§€)
try:
    from transformers import T5TokenizerFast, T5ForConditionalGeneration
    import torch

    HF_T5_AVAILABLE = True
except ImportError:
    HF_T5_AVAILABLE = False


class T5Normalizer:
    """T5 ê¸°ë°˜ í…ìŠ¤íŠ¸ ì •ê·œí™” (ë§ˆìŠ¤í‚¹ ìœ ì§€)"""

    # ë§ˆìŠ¤í¬ ë³´í˜¸ìš© íŒ¨í„´/ì„¼í‹°ë„¬
    MASK_PATTERN = re.compile(r"\[(?:[A-Zê°€-í£_]+)\]")
    SENTINEL_PREFIX = "<KEEP_"
    SENTINEL_SUFFIX = ">"

    def __init__(self, model_name: str = "j5ng/et5-typos-corrector"):
        if not HF_T5_AVAILABLE:
            raise ImportError(
                "Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”:\n  pip install transformers torch\n"
            )

        print("\n" + "=" * 70)
        print("ğŸ¤– T5 ê¸°ë°˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™” (ì•ˆì „ í”„ë¡¬í”„íŠ¸/ë§ˆìŠ¤í¬ ë³´í˜¸/í† í° ë¶„í• )")
        print("=" * 70)
        print(f"âœ“ ëª¨ë¸: {model_name}")
        print("  - Transformer ê¸°ë°˜ (Google T5)")
        print("  - ë§ì¶¤ë²•/ë„ì–´ì“°ê¸°/ê°„ë‹¨ ë¬¸ë²• êµì •")
        print("  - ğŸ¯ ë§ˆìŠ¤í¬ ìœ ì§€ (ì„¼í‹°ë„¬ ë³´í˜¸)")
        print("=" * 70)

        print("\n  ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ë‹¤ìš´ë¡œë“œ)")
        try:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"  ğŸ“ Device: {self.device.upper()}")

            self.tokenizer = T5TokenizerFast.from_pretrained(model_name)
            self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(
                self.device
            )
            self.model.eval()

            print(f"  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n")
            print("=" * 70 + "\n")
        except Exception as e:
            print(f"  âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    # ---------- ë§ˆìŠ¤í¬ ë³´í˜¸/ë³µì› ----------
    def _protect_masks(self, text: str):
        mapping = {}
        idx = 0

        def repl(m):
            nonlocal idx
            tok = f"{self.SENTINEL_PREFIX}{idx}{self.SENTINEL_SUFFIX}"
            mapping[tok] = m.group(0)
            idx += 1
            return tok

        return self.MASK_PATTERN.sub(repl, text), mapping

    def _restore_masks(self, text: str, mapping: Dict[str, str]):
        for k, v in mapping.items():
            text = text.replace(k, v)
        return text

    # ---------- í† í° ê¸¸ì´/ë¶„í•  ----------
    def _token_len(self, text: str) -> int:
        # special tokens í¬í•¨í•˜ì—¬ ì‹¤ì œ ìƒì„± í•œë„ ì²´í¬
        return len(self.tokenizer.encode(text, add_special_tokens=True))

    def _split_and_correct_by_tokens(
        self, text: str, max_tokens: int = 448, overlap: int = 64
    ) -> str:
        # special tokens ì œì™¸ë¡œ ê³ ì • ê¸¸ì´ ë¶„í• 
        ids = self.tokenizer.encode(text, add_special_tokens=False)
        out, start = [], 0
        while start < len(ids):
            end = min(len(ids), start + max_tokens)
            chunk_ids = ids[start:end]
            chunk_text = self.tokenizer.decode(chunk_ids)
            out.append(self._normalize_with_t5(chunk_text))  # ì¬ê·€
            if end == len(ids):
                break
            start = max(0, end - overlap)
        return "".join(out)

    # ---------- í¼ë¸”ë¦­ API ----------
    def normalize_chunk(self, chunk: Dict) -> Dict:
        """ì²­í¬ ì •ê·œí™” (ë§ˆìŠ¤í‚¹ ìœ ì§€)"""
        if not chunk.get("text"):
            return chunk

        original_text = chunk["text"]

        try:
            normalized_text = self._normalize_with_t5(original_text)

            chunk["text"] = normalized_text
            chunk["char_count"] = len(normalized_text)
            chunk["normalized"] = True
            chunk["original_length"] = len(original_text)

        except Exception as e:
            print(f"  âš ï¸ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            chunk["normalized"] = False
            chunk["normalization_error"] = str(e)

        return chunk

    def _normalize_with_t5(self, text: str) -> str:
        """T5 ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ êµì • (ì•ˆì „ í”„ë¡¬í”„íŠ¸ + ë§ˆìŠ¤í¬ ë³´í˜¸ + í† í° ë¶„í• )"""
        if not text or not text.strip():
            return text

        try:
            # 1) í† í° ê¸¸ì´ ê¸°ì¤€: ëª¨ë¸ í•œë„(512)ì— ì—¬ìœ ë¥¼ ë‘ê³  ë¶„í• 
            if self._token_len(text) > 448:
                return self._split_and_correct_by_tokens(
                    text, max_tokens=448, overlap=64
                )

            # 2) ë§ˆìŠ¤í¬ ë³´í˜¸(ì„¼í‹°ë„¬ë¡œ ì¹˜í™˜)
            protected, mapping = self._protect_masks(text)

            # 3) ì•ˆì „ í”„ë¡¬í”„íŠ¸(ì˜ë¯¸/ìˆ«ì/ê¸°í˜¸/ë§ˆìŠ¤í¬ ë¹„ë³€í˜•)
            input_text = (
                "ë‹¤ìŒ ë¬¸ì¥ì˜ ì² ìì™€ ë„ì–´ì“°ê¸°ë§Œ êµì •í•˜ê³ , ì˜ë¯¸/ìˆ«ì/ê¸°í˜¸/ë§ˆìŠ¤í¬ëŠ” ë°”ê¾¸ì§€ ë§ˆì„¸ìš”:\n"
                + protected
            )

            inputs = self.tokenizer(
                input_text,
                return_tensors="pt",
                max_length=512,
                truncation=True,
                padding=True,
            ).to(self.device)

            # 4) ì¶”ë¡  (ì•ˆì • ìš°ì„ : num_beams=1, fp16 ìë™ ìºìŠ¤íŠ¸)
            with torch.inference_mode():
                autocast_ctx = (
                    torch.cuda.amp.autocast
                    if self.device == "cuda"
                    else contextlib.nullcontext
                )
                with autocast_ctx():
                    outputs = self.model.generate(
                        **inputs,
                        max_length=512,
                        num_beams=1,  # í•„ìš” ì‹œ 3ìœ¼ë¡œ ì¡°ì •
                        early_stopping=True,
                        no_repeat_ngram_size=2,
                    )

            corrected = self.tokenizer.decode(
                outputs[0], skip_special_tokens=True
            ).strip()

            # 5) ë§ˆìŠ¤í¬ ë³µì›
            return self._restore_masks(corrected, mapping)

        except Exception as e:
            print(f"    T5 êµì • ì˜¤ë¥˜: {e}")
            return text

    # ê¸°ì¡´ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• ì€ ë³´ì¡°ë¡œ ìœ ì§€(í† í° ë¶„í•  ìš°ì„  ì ìš©)
    def _split_and_correct(self, text: str) -> str:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ êµì • (ë³´ì¡° ê²½ë¡œ)"""
        sentences = re.split(r"([.!?]\s+|\n\n)", text)
        corrected_parts = []
        current_batch = ""

        for i, segment in enumerate(sentences):
            if i % 2 == 1:  # êµ¬ë¶„ì
                corrected_parts.append(segment)
                continue

            segment = segment.strip()
            if not segment:
                continue

            if len(current_batch) + len(segment) > 300:
                if current_batch:
                    corrected = self._normalize_with_t5(current_batch)
                    corrected_parts.append(corrected)
                current_batch = segment
            else:
                current_batch += " " + segment if current_batch else segment

        if current_batch:
            corrected = self._normalize_with_t5(current_batch)
            corrected_parts.append(corrected)

        return "".join(corrected_parts)

    def normalize_all_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """ëª¨ë“  ì²­í¬ ì •ê·œí™”"""
        print(f"\n[T5 í…ìŠ¤íŠ¸ ì •ê·œí™”] ì´ {len(chunks)}ê°œ ì²­í¬")
        print(f"  ğŸ¯ ì•ˆì „ í”„ë¡¬í”„íŠ¸ (ë§ˆìŠ¤í¬ ìœ ì§€)")

        normalized_chunks = []
        success_count = 0
        error_count = 0
        total_time = 0

        for i, chunk in enumerate(chunks, 1):
            if chunk.get("text"):
                print(f"  ì²­í¬ {i}/{len(chunks)}...", end=" ")

                start = time.time()
                normalized = self.normalize_chunk(chunk)
                elapsed = time.time() - start

                if normalized.get("normalized"):
                    success_count += 1
                    total_time += elapsed

                    original_len = normalized.get("original_length", 0)
                    new_len = normalized.get("char_count", 0)
                    diff = new_len - original_len
                    diff_str = f"{diff:+d}" if diff != 0 else "Â±0"

                    print(f"âœ“ ({elapsed:.2f}ì´ˆ, {diff_str}ì)")
                else:
                    error_count += 1
                    print(f"âœ—")

                normalized_chunks.append(normalized)
            else:
                normalized_chunks.append(chunk)

        print(f"\n  âœ… ì •ê·œí™” ì™„ë£Œ:")
        print(f"     ì„±ê³µ: {success_count}/{len(chunks)}ê°œ")
        if error_count > 0:
            print(f"     ì‹¤íŒ¨: {error_count}ê°œ")
        if success_count > 0:
            print(f"     ì´ ì†Œìš”: {total_time:.2f}ì´ˆ")
            print(f"     í‰ê· : {total_time/success_count:.2f}ì´ˆ/ì²­í¬")

        return normalized_chunks


def normalize_with_t5(chunks: List[Dict]) -> List[Dict]:
    """
    íŒŒì´í”„ë¼ì¸ì— T5 ì •ê·œí™” í†µí•©

    Args:
        chunks: ë§ˆìŠ¤í‚¹ ì™„ë£Œëœ ì²­í¬ë“¤

    Returns:
        ì •ê·œí™”ëœ ì²­í¬ë“¤ (ë§ˆìŠ¤í¬ ìœ ì§€)
    """
    try:
        normalizer = T5Normalizer(model_name="j5ng/et5-typos-corrector")
        return normalizer.normalize_all_chunks(chunks)
    except ImportError:
        print(f"\nâš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(f"   ì„¤ì¹˜: pip install transformers torch")
        print(f"   ì •ê·œí™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.\n")
        return chunks
    except Exception as e:
        print(f"\nâš ï¸ T5 ì •ê·œí™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print(f"   ì •ê·œí™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.\n")
        return chunks
