"""
T5 ê¸°ë°˜ í•œêµ­ì–´ ë§ì¶¤ë²• êµì • ëª¨ë“ˆ (í”„ë¡¬í”„íŠ¸ ê°œì„  ë²„ì „)
ë§ˆìŠ¤í‚¹ ë³´í˜¸ ê°•í™”
back/scripts/normalize/ai_normalizer.py
"""

from typing import Dict, List
import time
import re

try:
    from transformers import T5TokenizerFast, T5ForConditionalGeneration
    import torch

    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False


class T5Normalizer:
    """T5 ê¸°ë°˜ í…ìŠ¤íŠ¸ ì •ê·œí™” (ë§ˆìŠ¤í‚¹ ë³´í˜¸)"""

    def __init__(self, model_name: str = "j5ng/et5-typos-corrector"):
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”:\n" "  pip install transformers torch\n"
            )

        print("\n" + "=" * 70)
        print("ğŸ¤– T5 ê¸°ë°˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™” (í”„ë¡¬í”„íŠ¸ ê°œì„ )")
        print("=" * 70)
        print(f"âœ“ ëª¨ë¸: {model_name}")
        print("  - Transformer ê¸°ë°˜ (Google T5)")
        print("  - ë§ì¶¤ë²• êµì •")
        print("  - ë„ì–´ì“°ê¸° êµì •")
        print("  - ë¬¸ë²• êµì •")
        print("  - ğŸ¯ ë§ˆìŠ¤í¬ ë³´í˜¸ (í”„ë¡¬í”„íŠ¸ ê°œì„ )")
        print("=" * 70)

        print("\n  ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ë‹¤ìš´ë¡œë“œ)")
        try:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"  ğŸ“ Device: {self.device.upper()}")

            self.tokenizer = T5TokenizerFast.from_pretrained(model_name)
            self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(
                self.device
            )
            self.model.eval()

            print(f"  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n")
            print("=" * 70 + "\n")
        except Exception as e:
            print(f"  âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def normalize_chunk(self, chunk: Dict) -> Dict:
        """ì²­í¬ ì •ê·œí™” (ë§ˆìŠ¤í‚¹ ìœ ì§€)"""
        if not chunk.get("text"):
            return chunk

        original_text = chunk["text"]

        try:
            # T5ë¡œ í…ìŠ¤íŠ¸ ì •ê·œí™” (í”„ë¡¬í”„íŠ¸ ê°œì„ ìœ¼ë¡œ ë§ˆìŠ¤í¬ ë³´í˜¸)
            normalized_text = self._normalize_with_t5(original_text)

            chunk["text"] = normalized_text
            chunk["char_count"] = len(normalized_text)
            chunk["normalized"] = True
            chunk["original_length"] = len(original_text)

        except Exception as e:
            print(f"  âš ï¸ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            chunk["normalized"] = False
            chunk["normalization_error"] = str(e)

        return chunk

    def _normalize_with_t5(self, text: str) -> str:
        """T5 ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ êµì • (í”„ë¡¬í”„íŠ¸ ê°œì„ )"""
        if not text or not text.strip():
            return text

        try:
            # 512 í† í° ì œí•œ ì²˜ë¦¬
            if len(text) > 400:
                return self._split_and_correct(text)

            # ğŸ†• í”„ë¡¬í”„íŠ¸ ê°œì„ : íŠ¹ìˆ˜ í† í° ë³´í˜¸ ëª…ì‹œ
            input_text = (
                f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ë„ì–´ì“°ê¸°ë§Œ êµì •í•˜ì„¸ìš”. "
                f"ëŒ€ê´„í˜¸ [ ] ì•ˆì˜ ë‹¨ì–´ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”: {text}"
            )

            inputs = self.tokenizer(
                input_text,
                return_tensors="pt",
                max_length=512,
                truncation=True,
                padding=True,
            ).to(self.device)

            # ğŸ†• ìƒì„± íŒŒë¼ë¯¸í„° ê°œì„ 
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=512,
                    num_beams=5,  # ğŸ†• 3 â†’ 5 (ë” ì •í™•)
                    early_stopping=True,
                    no_repeat_ngram_size=3,  # ğŸ†• 2 â†’ 3 (ë°˜ë³µ ë°©ì§€ ê°•í™”)
                    temperature=0.3,  # ğŸ†• ë³´ìˆ˜ì  ìƒì„±
                    do_sample=False,  # ğŸ†• í™•ì •ì  ìƒì„±
                )

            # ë””ì½”ë”©
            corrected = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            return corrected.strip()

        except Exception as e:
            print(f"    T5 êµì • ì˜¤ë¥˜: {e}")
            return text

    def _split_and_correct(self, text: str) -> str:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ êµì •"""
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r"([.!?]\s+|\n\n)", text)

        corrected_parts = []
        current_batch = ""

        for i, segment in enumerate(sentences):
            # êµ¬ë¶„ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            if i % 2 == 1:  # êµ¬ë¶„ì
                corrected_parts.append(segment)
                continue

            segment = segment.strip()
            if not segment:
                continue

            # ë°°ì¹˜ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì²˜ë¦¬
            if len(current_batch) + len(segment) > 300:
                if current_batch:
                    corrected = self._normalize_with_t5(current_batch)
                    corrected_parts.append(corrected)
                current_batch = segment
            else:
                current_batch += " " + segment if current_batch else segment

        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
        if current_batch:
            corrected = self._normalize_with_t5(current_batch)
            corrected_parts.append(corrected)

        return "".join(corrected_parts)

    def normalize_all_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """ëª¨ë“  ì²­í¬ ì •ê·œí™”"""
        print(f"\n[T5 í…ìŠ¤íŠ¸ ì •ê·œí™”] ì´ {len(chunks)}ê°œ ì²­í¬")
        print(f"  ğŸ¯ í”„ë¡¬í”„íŠ¸ ê°œì„  (ë§ˆìŠ¤í¬ ë³´í˜¸ ê°•í™”)")

        normalized_chunks = []
        success_count = 0
        error_count = 0
        total_time = 0

        for i, chunk in enumerate(chunks, 1):
            if chunk.get("text"):
                print(f"  ì²­í¬ {i}/{len(chunks)}...", end=" ")

                start = time.time()
                normalized = self.normalize_chunk(chunk)
                elapsed = time.time() - start

                if normalized.get("normalized"):
                    success_count += 1
                    total_time += elapsed

                    # ë³€í™”ëŸ‰ í‘œì‹œ
                    original_len = normalized.get("original_length", 0)
                    new_len = normalized.get("char_count", 0)
                    diff = new_len - original_len
                    diff_str = f"{diff:+d}" if diff != 0 else "Â±0"

                    print(f"âœ“ ({elapsed:.2f}ì´ˆ, {diff_str}ì)")
                else:
                    error_count += 1
                    print(f"âœ—")

                normalized_chunks.append(normalized)
            else:
                normalized_chunks.append(chunk)

        print(f"\n  âœ… ì •ê·œí™” ì™„ë£Œ:")
        print(f"     ì„±ê³µ: {success_count}/{len(chunks)}ê°œ")
        if error_count > 0:
            print(f"     ì‹¤íŒ¨: {error_count}ê°œ")
        if success_count > 0:
            print(f"     ì´ ì†Œìš”: {total_time:.2f}ì´ˆ")
            print(f"     í‰ê· : {total_time/success_count:.2f}ì´ˆ/ì²­í¬")

        return normalized_chunks


def normalize_with_t5(chunks: List[Dict]) -> List[Dict]:
    """
    íŒŒì´í”„ë¼ì¸ì— T5 ì •ê·œí™” í†µí•©

    Args:
        chunks: ë§ˆìŠ¤í‚¹ ì™„ë£Œëœ ì²­í¬ë“¤

    Returns:
        ì •ê·œí™”ëœ ì²­í¬ë“¤ (ë§ˆìŠ¤í¬ ë³´í˜¸)
    """
    try:
        normalizer = T5Normalizer(model_name="j5ng/et5-typos-corrector")
        return normalizer.normalize_all_chunks(chunks)
    except ImportError:
        print(f"\nâš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(f"   ì„¤ì¹˜: pip install transformers torch")
        print(f"   ì •ê·œí™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.\n")
        return chunks
    except Exception as e:
        print(f"\nâš ï¸ T5 ì •ê·œí™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print(f"   ì •ê·œí™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.\n")
        return chunks
