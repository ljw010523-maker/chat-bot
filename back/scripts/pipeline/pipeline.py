"""
ê°œì„ ëœ í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
Privacy Filter â†’ T5 Normalizer ë¶„ë¦¬ ì‹¤í–‰
ê°™ì€ ì œëª© íŒŒì¼ êµ¬ë¶„ ì €ì¥
HWP ê¹¨ì§„ í…ìŠ¤íŠ¸ í•„í„°ë§ ì¶”ê°€
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# ğŸ”¥ í˜„ì¬ ì„í¬íŠ¸ ì¤‘ì¸ íŒŒì¼ë“¤
from back.scripts.utils.config import Config  # ì„¤ì • íŒŒì¼
from back.scripts.ingest.document_loader import (
    UniversalDocumentLoader,
)  # ë¬¸ì„œ ë¡œë” (PDF, HWP, DOCX ë“±)
from back.scripts.clean.text_cleaner import TextCleaner  # í…ìŠ¤íŠ¸ ì •ì œê¸°
from back.scripts.chunk.semantic_splitter import HybridSemanticSplitter  # ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ë¶„í• ê¸°

import json
from datetime import datetime


class EnhancedUniversalPipeline:
    """Privacy â†’ T5 ìˆœì°¨ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        config: Config,
        use_ner_model: bool = True,
        ner_model_name: str = None,
        confidence_threshold: float = 0.6,
    ):
        self.config = config
        self.doc_loader = UniversalDocumentLoader(config)

        if ner_model_name is None:
            ner_model_name = "soddokayo/klue-roberta-large-klue-ner"
            print(f"\nğŸ¯ KLUE ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì„ íƒ: {ner_model_name}")
            print(
                f"   F1: 0.836 | Precision: 0.829 | Recall: 0.844 | Accuracy: 96.6%\n"
            )

        # Privacy Filter ì§ì ‘ ì´ˆê¸°í™”
        if config.use_privacy_filter:
            from back.scripts.clean.privacy_filter import PrivacyFilter

            self.privacy_filter = PrivacyFilter(
                use_ner_model=use_ner_model,
                ner_model_name=ner_model_name,
                use_gliner=True,
            )
            self.confidence_threshold = confidence_threshold
        else:
            self.privacy_filter = None

        # Text Cleaner (ê¸°ë³¸ ì •ì œë§Œ)
        self.text_cleaner = TextCleaner()

        self.text_splitter = HybridSemanticSplitter(config)

        self.output_folder = Path(config.output_folder)
        self.output_folder.mkdir(parents=True, exist_ok=True)

    def _is_valid_text_chunk(self, text: str) -> bool:
        """ì²­í¬ê°€ ìœ íš¨í•œ í…ìŠ¤íŠ¸ì¸ì§€ ê²€ì¦ (ê¹¨ì§„ í…ìŠ¤íŠ¸ í•„í„°ë§)"""
        if not text or len(text.strip()) < 10:
            return False

        # ì¶œë ¥ ê°€ëŠ¥í•œ ë¬¸ìë§Œ ì¶”ì¶œ
        printable_chars = [c for c in text if c.isprintable() and c not in "\n\t "]

        if not printable_chars or len(printable_chars) < 10:
            return False

        # í•œê¸€ ë¬¸ì ê°œìˆ˜
        korean_chars = sum(1 for c in printable_chars if "ê°€" <= c <= "í£")

        # ì˜ë¬¸ + ìˆ«ì ê°œìˆ˜
        alnum_chars = sum(1 for c in printable_chars if c.isalnum())

        # ì˜ë¯¸ìˆëŠ” ë¬¸ì ë¹„ìœ¨
        meaningful_ratio = (korean_chars + alnum_chars) / len(printable_chars)

        # í•œê¸€ì´ 5% ì´ìƒì´ê±°ë‚˜, ì˜ë¬¸/ìˆ«ìê°€ 30% ì´ìƒì´ë©´ ìœ íš¨
        korean_ratio = korean_chars / len(printable_chars)
        alnum_ratio = alnum_chars / len(printable_chars)

        # ê¹¨ì§„ í…ìŠ¤íŠ¸ëŠ” ì˜ë¯¸ìˆëŠ” ë¬¸ìê°€ ì ìŒ
        return meaningful_ratio >= 0.3 or korean_ratio >= 0.05 or alnum_ratio >= 0.3

    def process_document(self, doc_path: Path, save_privacy_report: bool = True):
        """ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬"""
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {doc_path.name}")
        print(f"{'='*60}")

        # 1. ë¬¸ì„œ ë¡œë“œ
        print("\n[1ë‹¨ê³„] ë¬¸ì„œ ë¡œë“œ")
        try:
            pages_data = self.doc_loader.load(doc_path)
        except Exception as e:
            print(f"  âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

        if not pages_data:
            print("  âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨!")
            return None

        # HWP íŒŒì¼ì€ ì´ì œ PDFë¡œ ë³€í™˜ë˜ì–´ ì²˜ë¦¬ë˜ë¯€ë¡œ ë³„ë„ ê²€ì¦ ë¶ˆí•„ìš”
        # (document_loaderì—ì„œ HWPâ†’PDF ë³€í™˜ í›„ ê¸°ì¡´ PDF ë¡œì§ ì‚¬ìš©)

        # 2. ê¸°ë³¸ ì •ì œë§Œ ìˆ˜í–‰ (ê³µë°± ì²˜ë¦¬ X)
        print("\n[2ë‹¨ê³„] ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ (íŠ¹ìˆ˜ë¬¸ìë§Œ)")
        for page in pages_data:
            if page.get("text"):
                page["text"] = self.text_cleaner.clean(page["text"])

        # 3. ì²­í¬ ë¶„í• 
        print("\n[3ë‹¨ê³„] ì²­í¬ ë¶„í• ")
        chunks = self.text_splitter.split(pages_data)
        print(f"  âœ“ ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        # HWP ì²­í¬ ì¬ê²€ì¦ ë¡œì§ ì œê±° (PDF ë³€í™˜ ë°©ì‹ìœ¼ë¡œ ì¸í•´ ë¶ˆí•„ìš”)

        # 4. Privacy Filter ì§ì ‘ ì ìš©
        print("\n[4ë‹¨ê³„] Privacy Filter (ê°œì¸ì •ë³´ë§Œ ì œê±°)")
        privacy_reports = []
        detection_methods_used = set()
        total_privacy_findings = 0

        if self.privacy_filter:
            for chunk in chunks:
                if not chunk.get("text"):
                    continue

                result = self.privacy_filter.filter_text(
                    chunk["text"],
                    confidence_threshold=self.confidence_threshold,
                    gliner_confidence=0.5,
                )

                chunk["text"] = result["filtered_text"]
                chunk["char_count"] = len(result["filtered_text"])

                if result["changes_made"]:
                    chunk["privacy_filtered"] = True
                    chunk["privacy_items"] = len(result["found_items"])

                    total_privacy_findings += chunk["privacy_items"]
                    detection_methods_used.update(result["detection_methods"])

                    page_num = chunk["page_num"]
                    existing_report = next(
                        (r for r in privacy_reports if r["page"] == page_num), None
                    )

                    if existing_report:
                        existing_report["findings"].extend(result["found_items"])
                    else:
                        privacy_reports.append(
                            {
                                "page": page_num,
                                "findings": result["found_items"],
                                "methods": result["detection_methods"],
                            }
                        )

            print(f"  âœ… í•„í„°ë§ ì™„ë£Œ:")
            print(f"     ì´ ì œê±°: {total_privacy_findings}ê±´")
            print(f"     ì‚¬ìš© ëª¨ë¸: {', '.join(detection_methods_used)}")

            filtered_chunks = [c for c in chunks if c.get("privacy_filtered")]
            print(f"     í•„í„°ë§ëœ ì²­í¬: {len(filtered_chunks)}/{len(chunks)}ê°œ")
        else:
            print(f"  âŠ˜ Privacy Filter ë¹„í™œì„±í™”")

        # 5. T5 í…ìŠ¤íŠ¸ ì •ê·œí™”
        print("\n[5ë‹¨ê³„] T5 í…ìŠ¤íŠ¸ ì •ê·œí™”")
        if self.config.use_hanspell_normalization:
            try:
                from back.scripts.normalize.ai_normalizer import normalize_with_t5

                chunks = normalize_with_t5(chunks)
            except ImportError:
                print(f"  âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜")
                print(f"     ì„¤ì¹˜: pip install transformers torch")
                print(f"     ì •ê·œí™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"  âš ï¸ T5 ì •ê·œí™” ì‹¤íŒ¨: {e}")
                print(f"     ì •ê·œí™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        else:
            print(f"  âŠ˜ T5 ì •ê·œí™” ë¹„í™œì„±í™”ë¨")

        # 6. í†µê³„ ê³„ì‚°
        total_chars = sum(chunk.get("char_count", 0) for chunk in chunks)
        non_empty = sum(1 for c in chunks if c.get("text"))
        avg_size = total_chars / non_empty if non_empty > 0 else 0

        print(f"\n[6ë‹¨ê³„] ìµœì¢… ê²°ê³¼")
        print(f"  âœ“ í…ìŠ¤íŠ¸ ìˆëŠ” ì²­í¬: {non_empty}ê°œ")
        print(f"  âœ“ ì´ ì¶”ì¶œ ë¬¸ì: {total_chars:,} ì")
        print(f"  âœ“ í‰ê·  ì²­í¬ í¬ê¸°: {avg_size:.0f} ì")

        # 7. ê²°ê³¼ ì €ì¥ (íŒŒì¼ íƒ€ì… êµ¬ë¶„)
        output_data = {
            "source_file": doc_path.name,
            "file_type": doc_path.suffix,
            "processed_at": datetime.now().isoformat(),
            "total_pages": len(pages_data),
            "total_chunks": len(chunks),
            "total_characters": total_chars,
            "average_chunk_size": round(avg_size, 2),
            "chunks": chunks,
            "processing_info": {
                "chunk_size": self.config.chunk_size,
                "chunk_overlap": self.config.chunk_overlap,
                "split_method": "langchain" if self.config.use_langchain else "basic",
                "methods_used": list(set(p["method"] for p in pages_data)),
                "privacy_filtering": {
                    "enabled": self.config.use_privacy_filter,
                    "model": "soddokayo/klue-roberta-large-klue-ner",
                    "model_performance": {
                        "f1": 0.836,
                        "precision": 0.829,
                        "recall": 0.844,
                        "accuracy": 0.966,
                    },
                    "detection_methods": list(detection_methods_used),
                    "total_findings": total_privacy_findings,
                    "chunks_affected": len(
                        [c for c in chunks if c.get("privacy_filtered")]
                    ),
                },
                "t5_normalization": {
                    "enabled": self.config.use_hanspell_normalization,
                    "model": "j5ng/et5-typos-corrector",
                    "chunks_normalized": len(
                        [c for c in chunks if c.get("normalized")]
                    ),
                },
            },
        }

        if save_privacy_report and privacy_reports:
            output_data["privacy_report"] = privacy_reports

        # ğŸ†• íŒŒì¼ëª…ì— í™•ì¥ì í¬í•¨ (ì¤‘ë³µ ë°©ì§€)
        file_stem = doc_path.stem
        file_ext = doc_path.suffix.lstrip(".")  # '.pdf' -> 'pdf'
        output_filename = f"{file_stem}_{file_ext}_chunks.json"
        output_path = self.output_folder / output_filename

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\n  âœ“ ì €ì¥ ì™„ë£Œ: {output_path}")

        if save_privacy_report and privacy_reports:
            report_filename = f"{file_stem}_{file_ext}_privacy_report.json"
            report_path = self.output_folder / report_filename

            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "source_file": doc_path.name,
                        "processed_at": datetime.now().isoformat(),
                        "model_info": {
                            "name": "soddokayo/klue-roberta-large-klue-ner",
                            "f1_score": 0.836,
                            "precision": 0.829,
                            "recall": 0.844,
                            "accuracy": 0.966,
                        },
                        "detection_methods": list(detection_methods_used),
                        "total_findings": total_privacy_findings,
                        "reports": privacy_reports,
                    },
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
            print(f"  âœ“ í”„ë¼ì´ë²„ì‹œ ë¦¬í¬íŠ¸: {report_path}")

        return output_data

    def process_all(self, save_privacy_reports: bool = True):
        """í´ë” ë‚´ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬"""
        raw_folder = Path(self.config.raw_folder)

        patterns = [
            "*.pdf",
            "*.txt",
            "*.docx",
            "*.doc",
            "*.pptx",
            "*.ppt",
            "*.jpg",
            "*.jpeg",
            "*.png",
            "*.xlsx",
            "*.xls",
            "*.csv",
            "*.hwp",
            "*.hwpx",
        ]

        all_files = []
        for pattern in patterns:
            all_files.extend(raw_folder.glob(pattern))

        if not all_files:
            print(f"\nâŒ ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {raw_folder.absolute()}")
            return []

        print(f"\n{'='*60}")
        print(f"ğŸš€ Privacy â†’ T5 ìˆœì°¨ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
        print(f"{'='*60}")
        print(f"ğŸ“ ëŒ€ìƒ í´ë”: {raw_folder.absolute()}")
        print(f"ğŸ“Š ë°œê²¬ëœ íŒŒì¼: {len(all_files)}ê°œ")

        # ê°™ì€ ì´ë¦„ íŒŒì¼ ê·¸ë£¹í™” í‘œì‹œ
        file_groups = {}
        for f in all_files:
            stem = f.stem
            if stem not in file_groups:
                file_groups[stem] = []
            file_groups[stem].append(f.suffix)

        duplicates = {k: v for k, v in file_groups.items() if len(v) > 1}
        if duplicates:
            print(f"\nâš ï¸ ê°™ì€ ì´ë¦„, ë‹¤ë¥¸ í™•ì¥ì íŒŒì¼ ë°œê²¬:")
            for name, exts in duplicates.items():
                exts_str = ", ".join(exts)
                print(f"   - {name}: {exts_str}")
            print(f"   â†’ ê°ê° 'íŒŒì¼ëª…_í™•ì¥ì_chunks.json' ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤")

        print(f"{'='*60}")

        results = []
        success_count = 0
        total_privacy_findings = 0

        for idx, doc_file in enumerate(all_files, 1):
            print(f"\n[{idx}/{len(all_files)}]")
            try:
                result = self.process_document(doc_file, save_privacy_reports)
                if result:
                    results.append(result)
                    success_count += 1

                    if self.config.use_privacy_filter:
                        total_privacy_findings += result["processing_info"][
                            "privacy_filtering"
                        ]["total_findings"]
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback

                traceback.print_exc()

        print(f"\n{'='*60}")
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ì„±ê³µ: {success_count}/{len(all_files)}ê°œ")
        print(f"  ì‹¤íŒ¨: {len(all_files) - success_count}ê°œ")

        if results:
            total_chunks = sum(r["total_chunks"] for r in results)
            total_chars = sum(r["total_characters"] for r in results)
            print(f"  ì´ ìƒì„± ì²­í¬: {total_chunks}ê°œ")
            print(f"  ì´ ì¶”ì¶œ ë¬¸ì: {total_chars:,}ì")

            if self.config.use_privacy_filter:
                print(f"  ğŸ¯ Privacy ì œê±°: {total_privacy_findings}ê±´")

            if self.config.use_hanspell_normalization:
                total_normalized = sum(
                    r["processing_info"]["t5_normalization"]["chunks_normalized"]
                    for r in results
                )
                print(f"  ğŸ¤– T5 ì •ê·œí™”: {total_normalized}ê°œ ì²­í¬")

        print(f"  ì €ì¥ ìœ„ì¹˜: {self.output_folder.absolute()}")
        print(f"{'='*60}\n")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    config = Config()

    print("\n" + "=" * 60)
    print("ğŸ¯ Privacy Filter â†’ T5 Normalizer íŒŒì´í”„ë¼ì¸")
    print("=" * 60)
    print("1ë‹¨ê³„: Privacy Filter (ê°œì¸ì •ë³´ë§Œ ì œê±°)")
    print("   - KLUE: soddokayo/klue-roberta-large-klue-ner")
    print("   - GLiNER: taeminlee/gliner_ko")
    print("2ë‹¨ê³„: T5 Normalizer (ê³µë°±/ë§ì¶¤ë²•/ë¬¸ë²•)")
    print("   - T5: j5ng/et5-typos-corrector")
    print("=" * 60 + "\n")

    pipeline = EnhancedUniversalPipeline(
        config=config,
        use_ner_model=True,
        ner_model_name=None,
        confidence_threshold=0.6,
    )

    pipeline.process_all(save_privacy_reports=True)


if __name__ == "__main__":
    main()
