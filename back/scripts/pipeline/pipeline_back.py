"""
ê°œì„ ëœ í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
Privacy Filter â†’ T5 Normalizer ë¶„ë¦¬ ì‹¤í–‰
"""

import sys
from pathlib import Path

project_root = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from back.scripts.utils.config import Config
from back.scripts.ingest.document_loader import UniversalDocumentLoader
from back.scripts.clean.text_cleaner import TextCleaner
from back.scripts.chunk.text_splitter import HybridTextSplitter
import json
from datetime import datetime


class EnhancedUniversalPipeline:
    """Privacy â†’ T5 ìˆœì°¨ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        config: Config,
        use_ner_model: bool = True,
        ner_model_name: str = None,
        confidence_threshold: float = 0.6,
    ):
        self.config = config
        self.doc_loader = UniversalDocumentLoader(config)

        if ner_model_name is None:
            ner_model_name = "soddokayo/klue-roberta-large-klue-ner"
            print(f"\nğŸ¯ KLUE ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì„ íƒ: {ner_model_name}")
            print(
                f"   F1: 0.836 | Precision: 0.829 | Recall: 0.844 | Accuracy: 96.6%\n"
            )

        # Privacy Filter ì§ì ‘ ì´ˆê¸°í™”
        if config.use_privacy_filter:
            from back.scripts.clean.privacy_filter import PrivacyFilter

            self.privacy_filter = PrivacyFilter(
                use_ner_model=use_ner_model,
                ner_model_name=ner_model_name,
                use_gliner=True,
            )
            self.confidence_threshold = confidence_threshold
        else:
            self.privacy_filter = None

        # Text Cleaner (ê¸°ë³¸ ì •ì œë§Œ)
        self.text_cleaner = TextCleaner(
            use_privacy_filter=False,  # PrivacyëŠ” ì§ì ‘ ì²˜ë¦¬
            use_ner_model=False,
            ner_model_name=None,
            confidence_threshold=confidence_threshold,
        )

        self.text_splitter = HybridTextSplitter(config)

        self.output_folder = Path(config.output_folder)
        self.output_folder.mkdir(parents=True, exist_ok=True)

    def process_document(self, doc_path: Path, save_privacy_report: bool = True):
        """ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬"""
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {doc_path.name}")
        print(f"{'='*60}")

        # 1. ë¬¸ì„œ ë¡œë“œ
        print("\n[1ë‹¨ê³„] ë¬¸ì„œ ë¡œë“œ")
        try:
            pages_data = self.doc_loader.load(doc_path)
        except Exception as e:
            print(f"  âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

        if not pages_data:
            print("  âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨!")
            return None

        # 2. ê¸°ë³¸ ì •ì œë§Œ ìˆ˜í–‰ (ê³µë°± ì²˜ë¦¬ X)
        print("\n[2ë‹¨ê³„] ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ (íŠ¹ìˆ˜ë¬¸ìë§Œ)")
        for page in pages_data:
            page["text"] = self.text_cleaner.clean(page["text"])

        # 3. ì²­í¬ ë¶„í• 
        print("\n[3ë‹¨ê³„] ì²­í¬ ë¶„í• ")
        chunks = self.text_splitter.split(pages_data)
        print(f"  âœ“ ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        # 4. ğŸ”§ Privacy Filter ì§ì ‘ ì ìš© (ê³µë°± ì²˜ë¦¬ X)
        print("\n[4ë‹¨ê³„] Privacy Filter (ê°œì¸ì •ë³´ë§Œ ì œê±°)")
        privacy_reports = []
        detection_methods_used = set()
        total_privacy_findings = 0

        if self.privacy_filter:
            for i, chunk in enumerate(chunks, 1):
                if not chunk.get("text"):
                    continue

                # Privacy Filter ì§ì ‘ í˜¸ì¶œ
                result = self.privacy_filter.filter_text(
                    chunk["text"],
                    confidence_threshold=self.confidence_threshold,
                    gliner_confidence=0.5,
                )

                # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë¡œ êµì²´
                chunk["text"] = result["filtered_text"]
                chunk["char_count"] = len(result["filtered_text"])

                # í†µê³„ ìˆ˜ì§‘
                if result["changes_made"]:
                    chunk["privacy_filtered"] = True
                    chunk["privacy_items"] = len(result["found_items"])

                    total_privacy_findings += chunk["privacy_items"]
                    detection_methods_used.update(result["detection_methods"])

                    # í˜ì´ì§€ë³„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
                    page_num = chunk["page_num"]
                    existing_report = next(
                        (r for r in privacy_reports if r["page"] == page_num), None
                    )

                    if existing_report:
                        existing_report["findings"].extend(result["found_items"])
                    else:
                        privacy_reports.append(
                            {
                                "page": page_num,
                                "findings": result["found_items"],
                                "methods": result["detection_methods"],
                            }
                        )

            print(f"  âœ… í•„í„°ë§ ì™„ë£Œ:")
            print(f"     ì´ ì œê±°: {total_privacy_findings}ê±´")
            print(f"     ì‚¬ìš© ëª¨ë¸: {', '.join(detection_methods_used)}")

            filtered_chunks = [c for c in chunks if c.get("privacy_filtered")]
            print(f"     í•„í„°ë§ëœ ì²­í¬: {len(filtered_chunks)}/{len(chunks)}ê°œ")
        else:
            print(f"  âŠ˜ Privacy Filter ë¹„í™œì„±í™”")

        # 5. ğŸ†• T5 í…ìŠ¤íŠ¸ ì •ê·œí™” (ê³µë°±/ë§ì¶¤ë²•/ë¬¸ë²•)
        print("\n[5ë‹¨ê³„] T5 í…ìŠ¤íŠ¸ ì •ê·œí™”")
        if self.config.use_hanspell_normalization:
            try:
                from back.scripts.normalize.ai_normalizer import normalize_with_t5

                chunks = normalize_with_t5(chunks)
            except ImportError:
                print(f"  âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜")
                print(f"     ì„¤ì¹˜: pip install transformers torch")
                print(f"     ì •ê·œí™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"  âš ï¸ T5 ì •ê·œí™” ì‹¤íŒ¨: {e}")
                print(f"     ì •ê·œí™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        else:
            print(f"  âŠ˜ T5 ì •ê·œí™” ë¹„í™œì„±í™”ë¨")

        # 6. í†µê³„ ê³„ì‚°
        total_chars = sum(chunk.get("char_count", 0) for chunk in chunks)
        non_empty = sum(1 for c in chunks if c.get("text"))
        avg_size = total_chars / non_empty if non_empty > 0 else 0

        print(f"\n[6ë‹¨ê³„] ìµœì¢… ê²°ê³¼")
        print(f"  âœ“ í…ìŠ¤íŠ¸ ìˆëŠ” ì²­í¬: {non_empty}ê°œ")
        print(f"  âœ“ ì´ ì¶”ì¶œ ë¬¸ì: {total_chars:,} ì")
        print(f"  âœ“ í‰ê·  ì²­í¬ í¬ê¸°: {avg_size:.0f} ì")

        # 7. ê²°ê³¼ ì €ì¥
        output_data = {
            "source_file": doc_path.name,
            "file_type": doc_path.suffix,
            "processed_at": datetime.now().isoformat(),
            "total_pages": len(pages_data),
            "total_chunks": len(chunks),
            "total_characters": total_chars,
            "average_chunk_size": round(avg_size, 2),
            "chunks": chunks,
            "processing_info": {
                "chunk_size": self.config.chunk_size,
                "chunk_overlap": self.config.chunk_overlap,
                "split_method": "langchain" if self.config.use_langchain else "basic",
                "methods_used": list(set(p["method"] for p in pages_data)),
                "privacy_filtering": {
                    "enabled": self.config.use_privacy_filter,
                    "model": "soddokayo/klue-roberta-large-klue-ner",
                    "model_performance": {
                        "f1": 0.836,
                        "precision": 0.829,
                        "recall": 0.844,
                        "accuracy": 0.966,
                    },
                    "detection_methods": list(detection_methods_used),
                    "total_findings": total_privacy_findings,
                    "chunks_affected": len(
                        [c for c in chunks if c.get("privacy_filtered")]
                    ),
                },
                "t5_normalization": {
                    "enabled": self.config.use_hanspell_normalization,
                    "model": "j5ng/et5-typos-corrector",
                    "chunks_normalized": len(
                        [c for c in chunks if c.get("normalized")]
                    ),
                },
            },
        }

        if save_privacy_report and privacy_reports:
            output_data["privacy_report"] = privacy_reports

        output_path = self.output_folder / f"{doc_path.stem}_chunks.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\n  âœ“ ì €ì¥ ì™„ë£Œ: {output_path}")

        if save_privacy_report and privacy_reports:
            report_path = self.output_folder / f"{doc_path.stem}_privacy_report.json"
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "source_file": doc_path.name,
                        "processed_at": datetime.now().isoformat(),
                        "model_info": {
                            "name": "soddokayo/klue-roberta-large-klue-ner",
                            "f1_score": 0.836,
                            "precision": 0.829,
                            "recall": 0.844,
                            "accuracy": 0.966,
                        },
                        "detection_methods": list(detection_methods_used),
                        "total_findings": total_privacy_findings,
                        "reports": privacy_reports,
                    },
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
            print(f"  âœ“ í”„ë¼ì´ë²„ì‹œ ë¦¬í¬íŠ¸: {report_path}")

        return output_data

    def process_all(self, save_privacy_reports: bool = True):
        """í´ë” ë‚´ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬"""
        raw_folder = Path(self.config.raw_folder)

        patterns = [
            "*.pdf",
            "*.txt",
            "*.docx",
            "*.doc",
            "*.pptx",
            "*.ppt",
            "*.jpg",
            "*.jpeg",
            "*.png",
            "*.xlsx",
            "*.xls",
            "*.csv",
        ]

        all_files = []
        for pattern in patterns:
            all_files.extend(raw_folder.glob(pattern))

        if not all_files:
            print(f"\nâŒ ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {raw_folder.absolute()}")
            return []

        print(f"\n{'='*60}")
        print(f"ğŸš€ Privacy â†’ T5 ìˆœì°¨ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
        print(f"{'='*60}")
        print(f"ğŸ“ ëŒ€ìƒ í´ë”: {raw_folder.absolute()}")
        print(f"ğŸ“Š ë°œê²¬ëœ íŒŒì¼: {len(all_files)}ê°œ")
        print(f"{'='*60}")

        results = []
        success_count = 0
        total_privacy_findings = 0

        for idx, doc_file in enumerate(all_files, 1):
            print(f"\n[{idx}/{len(all_files)}]")
            try:
                result = self.process_document(doc_file, save_privacy_reports)
                if result:
                    results.append(result)
                    success_count += 1

                    if self.config.use_privacy_filter:
                        total_privacy_findings += result["processing_info"][
                            "privacy_filtering"
                        ]["total_findings"]
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback

                traceback.print_exc()

        print(f"\n{'='*60}")
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ì„±ê³µ: {success_count}/{len(all_files)}ê°œ")
        print(f"  ì‹¤íŒ¨: {len(all_files) - success_count}ê°œ")

        if results:
            total_chunks = sum(r["total_chunks"] for r in results)
            total_chars = sum(r["total_characters"] for r in results)
            print(f"  ì´ ìƒì„± ì²­í¬: {total_chunks}ê°œ")
            print(f"  ì´ ì¶”ì¶œ ë¬¸ì: {total_chars:,}ì")

            if self.config.use_privacy_filter:
                print(f"  ğŸ¯ Privacy ì œê±°: {total_privacy_findings}ê±´")

            if self.config.use_hanspell_normalization:
                total_normalized = sum(
                    r["processing_info"]["t5_normalization"]["chunks_normalized"]
                    for r in results
                )
                print(f"  ğŸ¤– T5 ì •ê·œí™”: {total_normalized}ê°œ ì²­í¬")

        print(f"  ì €ì¥ ìœ„ì¹˜: {self.output_folder.absolute()}")
        print(f"{'='*60}\n")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    config = Config()

    print("\n" + "=" * 60)
    print("ğŸ¯ Privacy Filter â†’ T5 Normalizer íŒŒì´í”„ë¼ì¸")
    print("=" * 60)
    print("1ë‹¨ê³„: Privacy Filter (ê°œì¸ì •ë³´ë§Œ ì œê±°)")
    print("   - KLUE: soddokayo/klue-roberta-large-klue-ner")
    print("   - GLiNER: taeminlee/gliner_ko")
    print("2ë‹¨ê³„: T5 Normalizer (ê³µë°±/ë§ì¶¤ë²•/ë¬¸ë²•)")
    print("   - T5: j5ng/et5-typos-corrector")
    print("=" * 60 + "\n")

    pipeline = EnhancedUniversalPipeline(
        config=config,
        use_ner_model=True,
        ner_model_name=None,
        confidence_threshold=0.6,
    )

    pipeline.process_all(save_privacy_reports=True)


if __name__ == "__main__":
    main()
