"""
ê°„ì†Œí™”ëœ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ë¯¼ê°ì •ë³´ ì²˜ë¦¬ ì œê±°)
back/scripts/pipeline/simplified_pipeline.py
"""

import sys
from pathlib import Path

project_root = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from back.scripts.utils.config import Config
from back.scripts.ingest.document_loader import UniversalDocumentLoader
from back.scripts.clean.text_cleaner import TextCleaner
from back.scripts.chunk.text_splitter import HybridTextSplitter
import json
from datetime import datetime


class SimplifiedPipeline:
    """ê°„ì†Œí™”ëœ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config: Config):
        self.config = config
        self.doc_loader = UniversalDocumentLoader(config)
        self.text_cleaner = TextCleaner()
        self.text_splitter = HybridTextSplitter(config)

        self.output_folder = Path(config.output_folder)
        self.output_folder.mkdir(parents=True, exist_ok=True)

    def process_document(self, doc_path: Path):
        """ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬"""
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {doc_path.name}")
        print(f"{'='*60}")

        # 1. ë¬¸ì„œ ë¡œë“œ
        print("\n[1ë‹¨ê³„] ë¬¸ì„œ ë¡œë“œ")
        try:
            pages_data = self.doc_loader.load(doc_path)
        except Exception as e:
            print(f"  âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

        if not pages_data:
            print("  âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨!")
            return None

        # 2. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ (ê³µë°±ë§Œ)
        print("\n[2ë‹¨ê³„] í…ìŠ¤íŠ¸ ì •ì œ (ê³µë°± ì²˜ë¦¬)")
        for page in pages_data:
            page["text"] = self.text_cleaner.clean(page["text"])

        print(f"  âœ“ {len(pages_data)}ê°œ í˜ì´ì§€ ì •ì œ ì™„ë£Œ")

        # 3. ì²­í¬ ë¶„í• 
        print("\n[3ë‹¨ê³„] ì²­í¬ ë¶„í• ")
        chunks = self.text_splitter.split(pages_data)

        total_chars = sum(chunk.get("char_count", 0) for chunk in chunks)
        non_empty = sum(1 for c in chunks if c.get("text"))
        avg_size = total_chars / non_empty if non_empty > 0 else 0

        print(f"  âœ“ ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        print(f"  âœ“ í…ìŠ¤íŠ¸ ìˆëŠ” ì²­í¬: {non_empty}ê°œ")
        print(f"  âœ“ ì´ ì¶”ì¶œ ë¬¸ì: {total_chars:,} ì")
        print(f"  âœ“ í‰ê·  ì²­í¬ í¬ê¸°: {avg_size:.0f} ì")

        # 4. ê²°ê³¼ ì €ì¥
        output_data = {
            "source_file": doc_path.name,
            "file_type": doc_path.suffix,
            "processed_at": datetime.now().isoformat(),
            "total_pages": len(pages_data),
            "total_chunks": len(chunks),
            "total_characters": total_chars,
            "average_chunk_size": round(avg_size, 2),
            "chunks": chunks,
            "processing_info": {
                "chunk_size": self.config.chunk_size,
                "chunk_overlap": self.config.chunk_overlap,
                "split_method": "langchain" if self.config.use_langchain else "basic",
                "methods_used": list(set(p["method"] for p in pages_data)),
                "text_cleaning": "basic_whitespace_only",
            },
        }

        output_path = self.output_folder / f"{doc_path.stem}_chunks.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\n  âœ“ ì €ì¥ ì™„ë£Œ: {output_path}")
        return output_data

    def process_all(self):
        """í´ë” ë‚´ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬"""
        raw_folder = Path(self.config.raw_folder)

        patterns = [
            "*.pdf",
            "*.txt",
            "*.docx",
            "*.doc",
            "*.pptx",
            "*.ppt",
            "*.jpg",
            "*.jpeg",
            "*.png",
            "*.xlsx",
            "*.xls",
            "*.csv",
        ]

        all_files = []
        for pattern in patterns:
            all_files.extend(raw_folder.glob(pattern))

        if not all_files:
            print(f"\nâŒ ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {raw_folder.absolute()}")
            return []

        print(f"\n{'='*60}")
        print(f"ğŸš€ ê°„ì†Œí™”ëœ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ğŸ“ ëŒ€ìƒ í´ë”: {raw_folder.absolute()}")
        print(f"ğŸ“Š ë°œê²¬ëœ íŒŒì¼: {len(all_files)}ê°œ")
        print(f"âš™ï¸ ì²˜ë¦¬ ë°©ì‹: ê³µë°± ì •ì œë§Œ (ë¯¼ê°ì •ë³´ í•„í„°ë§ ì—†ìŒ)")
        print(f"{'='*60}")

        results = []
        success_count = 0

        for idx, doc_file in enumerate(all_files, 1):
            print(f"\n[{idx}/{len(all_files)}]")
            try:
                result = self.process_document(doc_file)
                if result:
                    results.append(result)
                    success_count += 1
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback

                traceback.print_exc()

        print(f"\n{'='*60}")
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ì„±ê³µ: {success_count}/{len(all_files)}ê°œ")
        print(f"  ì‹¤íŒ¨: {len(all_files) - success_count}ê°œ")

        if results:
            total_chunks = sum(r["total_chunks"] for r in results)
            total_chars = sum(r["total_characters"] for r in results)
            print(f"  ì´ ìƒì„± ì²­í¬: {total_chunks}ê°œ")
            print(f"  ì´ ì¶”ì¶œ ë¬¸ì: {total_chars:,}ì")

        print(f"  ì €ì¥ ìœ„ì¹˜: {self.output_folder.absolute()}")
        print(f"{'='*60}\n")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    config = Config()

    print("\n" + "=" * 60)
    print("ğŸ“ ê°„ì†Œí™”ëœ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
    print("=" * 60)
    print("âœ“ ë¬¸ì„œ ë¡œë“œ â†’ ê³µë°± ì •ì œ â†’ ì²­í¬ ë¶„í• ")
    print("âœ— ë¯¼ê°ì •ë³´ í•„í„°ë§ ë¹„í™œì„±í™”")
    print("âœ— T5 ì •ê·œí™” ë¹„í™œì„±í™”")
    print("=" * 60 + "\n")

    pipeline = SimplifiedPipeline(config)
    pipeline.process_all()


if __name__ == "__main__":
    main()
