"""
í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
"""

import sys
from pathlib import Path

project_root = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from back.scripts.utils.config import Config
from back.scripts.ingest.document_loader import UniversalDocumentLoader  # â† ë³€ê²½
from back.scripts.clean.text_cleaner import TextCleaner
from back.scripts.chunk.text_splitter import HybridTextSplitter
import json


class UniversalPipeline:
    """í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config: Config):
        self.config = config
        self.doc_loader = UniversalDocumentLoader(config)  # â† ë³€ê²½
        self.text_cleaner = TextCleaner(use_privacy_filter=config.use_privacy_filter)
        self.text_splitter = HybridTextSplitter(config)

        self.output_folder = Path(config.output_folder)
        self.output_folder.mkdir(parents=True, exist_ok=True)

    def process_document(self, doc_path: Path):
        """ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬ (ëª¨ë“  í˜•ì‹ ì§€ì›)"""
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {doc_path.name}")
        print(f"{'='*60}")

        # 1. ë¬¸ì„œ ë¡œë“œ (ìë™ í˜•ì‹ ê°ì§€)
        print("\n[1ë‹¨ê³„] ë¬¸ì„œ ë¡œë“œ")
        try:
            pages_data = self.doc_loader.load(doc_path)
        except Exception as e:
            print(f"  âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

        if not pages_data:
            print("  âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨!")
            return None

        # 2. í…ìŠ¤íŠ¸ ì •ì œ + AI ìë™ í•„í„°ë§
        print("\n[2ë‹¨ê³„] í…ìŠ¤íŠ¸ ì •ì œ ë° AI ìë™ í•„í„°ë§")
        privacy_reports = []

        for page in pages_data:
            if self.config.use_privacy_filter:
                result = self.text_cleaner.clean_with_privacy(page["text"])
                page["text"] = result["text"]

                if result["privacy_filtered"]:
                    privacy_reports.append(
                        {"page": page["page_num"], "findings": result["privacy_report"]}
                    )
                    print(
                        f"  ğŸ”’ í˜ì´ì§€ {page['page_num']}: ë¯¼ê°ì •ë³´ {len(result['privacy_report'])}ê±´ ìë™ ì œê±°"
                    )
            else:
                page["text"] = self.text_cleaner.clean(page["text"])

        if privacy_reports:
            total_findings = sum(
                sum(item["count"] for item in report["findings"])
                for report in privacy_reports
            )
            print(f"  âœ… AIê°€ ìë™ìœ¼ë¡œ ì´ {total_findings}ê±´ ì œê±° ì™„ë£Œ")

        # 3. ì²­í¬ ë¶„í• 
        print("\n[3ë‹¨ê³„] ì²­í¬ ë¶„í• ")
        chunks = self.text_splitter.split(pages_data)

        total_chars = sum(chunk.get("char_count", 0) for chunk in chunks)
        non_empty = sum(1 for c in chunks if c.get("text"))
        avg_size = total_chars / non_empty if non_empty > 0 else 0

        print(f"  âœ“ ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        print(f"  âœ“ í…ìŠ¤íŠ¸ ìˆëŠ” ì²­í¬: {non_empty}ê°œ")
        print(f"  âœ“ ì´ ì¶”ì¶œ ë¬¸ì: {total_chars:,} ì")
        print(f"  âœ“ í‰ê·  ì²­í¬ í¬ê¸°: {avg_size:.0f} ì")

        # 4. ê²°ê³¼ ì €ì¥
        output_data = {
            "source_file": doc_path.name,
            "file_type": doc_path.suffix,
            "total_pages": len(pages_data),
            "total_chunks": len(chunks),
            "total_characters": total_chars,
            "average_chunk_size": round(avg_size, 2),
            "chunks": chunks,
            "processing_info": {
                "chunk_size": self.config.chunk_size,
                "chunk_overlap": self.config.chunk_overlap,
                "split_method": "langchain" if self.config.use_langchain else "basic",
                "methods_used": list(set(p["method"] for p in pages_data)),
                "privacy_filtering": self.config.use_privacy_filter,
                "privacy_method": "ai_auto",
                "privacy_findings": privacy_reports if privacy_reports else None,
            },
        }

        output_path = self.output_folder / f"{doc_path.stem}_chunks.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\n  âœ“ ì €ì¥ ì™„ë£Œ: {output_path}")
        return output_data

    def process_all(self):
        """í´ë” ë‚´ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬"""
        raw_folder = Path(self.config.raw_folder)

        # ì§€ì› í˜•ì‹
        patterns = [
            "*.pdf",
            "*.txt",
            "*.docx",
            "*.doc",
            "*.pptx",
            "*.ppt",
            "*.jpg",
            "*.jpeg",
            "*.png",
            "*.xlsx",
            "*.xls",
            "*.csv",
        ]

        all_files = []
        for pattern in patterns:
            all_files.extend(raw_folder.glob(pattern))

        if not all_files:
            print(f"\nâŒ ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {raw_folder.absolute()}")
            return []

        print(f"\n{'='*60}")
        print(f"ğŸš€ í†µí•© ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ğŸ“ ëŒ€ìƒ í´ë”: {raw_folder.absolute()}")
        print(f"ğŸ“Š ë°œê²¬ëœ íŒŒì¼: {len(all_files)}ê°œ")
        print(f"{'='*60}")

        results = []
        success_count = 0

        for idx, doc_file in enumerate(all_files, 1):
            print(f"\n[{idx}/{len(all_files)}]")
            try:
                result = self.process_document(doc_file)
                if result:
                    results.append(result)
                    success_count += 1
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback

                traceback.print_exc()

        # ìµœì¢… ìš”ì•½
        print(f"\n{'='*60}")
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ì„±ê³µ: {success_count}/{len(all_files)}ê°œ")
        print(f"  ì‹¤íŒ¨: {len(all_files) - success_count}ê°œ")

        if results:
            total_chunks = sum(r["total_chunks"] for r in results)
            total_chars = sum(r["total_characters"] for r in results)
            print(f"  ì´ ìƒì„± ì²­í¬: {total_chunks}ê°œ")
            print(f"  ì´ ì¶”ì¶œ ë¬¸ì: {total_chars:,}ì")

        print(f"  ì €ì¥ ìœ„ì¹˜: {self.output_folder.absolute()}")
        print(f"{'='*60}\n")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    config = Config()
    pipeline = UniversalPipeline(config)
    pipeline.process_all()


if __name__ == "__main__":
    main()
