import os
import re
from pathlib import Path
from typing import List, Dict, Optional
import PyPDF2
from pdf2image import convert_from_path
import pytesseract
from PIL import Image
import json
import platform

# LangChain imports
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„í•  ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install langchain")

# ============================================
# Windows: Tesseract ë° Poppler ê²½ë¡œ ì„¤ì •
# ============================================
if platform.system() == 'Windows':
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
    
    # Poppler ê²½ë¡œ ìë™ ì°¾ê¸°
    import glob
    poppler_search = glob.glob(r'C:\Program Files\poppler-*\Library\bin')
    if poppler_search:
        POPPLER_PATH = poppler_search[0]
    else:
        POPPLER_PATH = r'C:\Program Files\poppler-25.07.0\Library\bin'
else:
    POPPLER_PATH = None


class HybridPDFProcessor:
    """
    í•˜ì´ë¸Œë¦¬ë“œ PDF ì²˜ë¦¬ê¸°
    - OCR: ì»¤ìŠ¤í…€ êµ¬í˜„ (Tesseract)
    - ì²­í¬ ë¶„í• : LangChain (ì˜µì…˜) ë˜ëŠ” ê¸°ë³¸ ë°©ì‹
    """
    
    def __init__(
        self, 
        raw_folder: str = "data/raw", 
        output_folder: str = "data/chunks",
        use_langchain: bool = True,
        chunk_size: int = 500,  # ìŠ¤ìº”ë³¸ì´ë¯€ë¡œ ì‘ê²Œ
        chunk_overlap: int = 50,  # ì˜¤ë²„ë© ì¤„ì„
        ocr_dpi: int = 300
    ):
        self.raw_folder = Path(raw_folder)
        self.output_folder = Path(output_folder)
        self.output_folder.mkdir(parents=True, exist_ok=True)
        
        self.use_langchain = use_langchain and LANGCHAIN_AVAILABLE
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.ocr_dpi = ocr_dpi
        
        # LangChain TextSplitter ì´ˆê¸°í™”
        if self.use_langchain:
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=[
                    "\n\n",  # ë¬¸ë‹¨ êµ¬ë¶„
                    "\n",    # ì¤„ë°”ê¿ˆ
                    "ã€‚",    # í•œêµ­ì–´ ë§ˆì¹¨í‘œ
                    ". ",    # ì˜ì–´ ë§ˆì¹¨í‘œ
                    "! ",    # ëŠë‚Œí‘œ
                    "? ",    # ë¬¼ìŒí‘œ
                    "ï¼Œ ",   # ì‰¼í‘œ
                    ", ",
                    " ",     # ê³µë°±
                    ""       # ë§ˆì§€ë§‰ ìˆ˜ë‹¨
                ],
                length_function=len,
                is_separator_regex=False
            )
            print(f"âœ“ LangChain ë¶„í•  ëª¨ë“œ í™œì„±í™” (chunk_size={chunk_size}, overlap={chunk_overlap})")
        else:
            print(f"âœ“ ê¸°ë³¸ ë¶„í•  ëª¨ë“œ (chunk_size={chunk_size}, overlap={chunk_overlap})")
    
    # ============================================
    # OCR ê´€ë ¨ ë©”ì„œë“œ (ì»¤ìŠ¤í…€ êµ¬í˜„ ìœ ì§€)
    # ============================================
    
    def extract_text_from_pdf(self, pdf_path: Path) -> List[Dict]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyPDF2 ìš°ì„ , ì‹¤íŒ¨ì‹œ OCR)"""
        pages_data = []
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                
                print(f"  ğŸ“‘ ì´ {total_pages}í˜ì´ì§€ ë°œê²¬")
                
                # ì²« í˜ì´ì§€ë¡œ ìŠ¤ìº”ë³¸ ì—¬ë¶€ íŒë‹¨
                first_page_text = pdf_reader.pages[0].extract_text().strip()
                
                if len(first_page_text) < 50:
                    print("  âš ï¸ ìŠ¤ìº”ë³¸ PDFë¡œ ê°ì§€ë¨ â†’ ì „ì²´ OCR ëª¨ë“œ")
                    return self._ocr_entire_pdf(pdf_path)
                
                # í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF - í˜ì´ì§€ë³„ ì²˜ë¦¬
                print("  âœ“ í…ìŠ¤íŠ¸ ê¸°ë°˜ PDFë¡œ ê°ì§€ë¨")
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    text = page.extract_text().strip()
                    
                    if len(text) < 50:
                        print(f"    í˜ì´ì§€ {page_num}: í…ìŠ¤íŠ¸ ë¶€ì¡± â†’ OCR ì‚¬ìš©")
                        text = self._ocr_page(pdf_path, page_num)
                        method = 'ocr'
                    else:
                        method = 'extraction'
                    
                    pages_data.append({
                        'page_num': page_num,
                        'text': text,
                        'method': method
                    })
                    
        except Exception as e:
            print(f"  âŒ PyPDF2 ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            print("  ğŸ”„ OCR ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
            pages_data = self._ocr_entire_pdf(pdf_path)
            
        return pages_data
    
    def _ocr_page(self, pdf_path: Path, page_num: int) -> str:
        """íŠ¹ì • í˜ì´ì§€ì— ëŒ€í•´ OCR ìˆ˜í–‰"""
        try:
            if platform.system() == 'Windows':
                images = convert_from_path(
                    pdf_path, 
                    first_page=page_num, 
                    last_page=page_num,
                    dpi=self.ocr_dpi,
                    poppler_path=POPPLER_PATH
                )
            else:
                images = convert_from_path(
                    pdf_path, 
                    first_page=page_num, 
                    last_page=page_num,
                    dpi=self.ocr_dpi
                )
            
            if images:
                # í•œê¸€+ì˜ì–´ OCR
                custom_config = r'--oem 3 --psm 6'
                text = pytesseract.image_to_string(
                    images[0], 
                    lang='kor+eng',
                    config=custom_config
                )
                
                if text.strip():
                    print(f"      âœ“ ì„±ê³µ (ì¶”ì¶œ: {len(text)} ì)")
                else:
                    print(f"      âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ")
                
                return text.strip()
            else:
                print(f"      âŒ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨")
                return ""
                
        except Exception as e:
            print(f"      âŒ OCR ì‹¤íŒ¨: {e}")
            return ""
    
    def _ocr_entire_pdf(self, pdf_path: Path) -> List[Dict]:
        """ì „ì²´ PDFì— ëŒ€í•´ OCR ìˆ˜í–‰"""
        pages_data = []
        
        try:
            print("  ğŸ” PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘...")
            
            if platform.system() == 'Windows':
                images = convert_from_path(
                    pdf_path, 
                    dpi=self.ocr_dpi,
                    poppler_path=POPPLER_PATH
                )
            else:
                images = convert_from_path(pdf_path, dpi=self.ocr_dpi)
            
            print(f"  âœ“ {len(images)}ê°œ í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ")
            print("  ğŸ”¤ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            
            for page_num, image in enumerate(images, 1):
                print(f"    í˜ì´ì§€ {page_num}/{len(images)}...", end=" ")
                
                try:
                    custom_config = r'--oem 3 --psm 6'
                    text = pytesseract.image_to_string(
                        image, 
                        lang='kor+eng',
                        config=custom_config
                    )
                    
                    text = text.strip()
                    
                    if text:
                        print(f"âœ“ (ì¶”ì¶œ: {len(text)} ì)")
                    else:
                        print("âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ")
                    
                    pages_data.append({
                        'page_num': page_num,
                        'text': text,
                        'method': 'ocr'
                    })
                    
                except Exception as e:
                    print(f"âŒ ì‹¤íŒ¨: {e}")
                    pages_data.append({
                        'page_num': page_num,
                        'text': '',
                        'method': 'ocr_failed'
                    })
                
        except Exception as e:
            print(f"  âŒ ì „ì²´ OCR ì‹¤íŒ¨: {e}")
            
        return pages_data
    
    # ============================================
    # í…ìŠ¤íŠ¸ ì •ì œ
    # ============================================
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ - OCR ë…¸ì´ì¦ˆ ì œê±°"""
        if not text:
            return ""
        
        # 1. ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r' +', ' ', text)
        
        # 2. íƒ­ ë¬¸ì ì œê±°
        text = text.replace('\t', ' ')
        
        # 3. ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ 2ê°œë¡œ ì œí•œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 4. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(line for line in lines if line)  # ë¹ˆ ì¤„ ì œê±°
        
        # 5. OCR íŠ¹ìˆ˜ ë…¸ì´ì¦ˆ ì œê±° (ì„ íƒì )
        # text = re.sub(r'[^\w\s\n.,!?()[\]{}\-:;\'\"ê°€-í£]', '', text)
        
        return text.strip()
    
    # ============================================
    # ì²­í¬ ë¶„í•  (í•˜ì´ë¸Œë¦¬ë“œ)
    # ============================================
    
    def split_into_chunks_langchain(self, pages_data: List[Dict]) -> List[Dict]:
        """LangChainì„ ì‚¬ìš©í•œ ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• """
        chunks = []
        chunk_id = 0
        
        for page_data in pages_data:
            text = self.clean_text(page_data['text'])
            page_num = page_data['page_num']
            
            if not text:
                chunks.append({
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': '',
                    'char_count': 0,
                    'warning': 'no_text_extracted'
                })
                chunk_id += 1
                continue
            
            # LangChainìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• 
            split_texts = self.text_splitter.split_text(text)
            
            for split_text in split_texts:
                chunks.append({
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': split_text,
                    'char_count': len(split_text),
                    'split_method': 'langchain'
                })
                chunk_id += 1
        
        return chunks
    
    def split_into_chunks_basic(self, pages_data: List[Dict]) -> List[Dict]:
        """ê¸°ë³¸ ì²­í¬ ë¶„í•  (í´ë°±)"""
        chunks = []
        chunk_id = 0
        
        for page_data in pages_data:
            text = self.clean_text(page_data['text'])
            page_num = page_data['page_num']
            
            if not text:
                chunks.append({
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': '',
                    'char_count': 0,
                    'warning': 'no_text_extracted'
                })
                chunk_id += 1
                continue
            
            # ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ
            if len(text) <= self.chunk_size:
                chunks.append({
                    'chunk_id': chunk_id,
                    'page_num': page_num,
                    'text': text,
                    'char_count': len(text),
                    'split_method': 'basic'
                })
                chunk_id += 1
            else:
                # ì˜¤ë²„ë© ë°©ì‹ ë¶„í• 
                start = 0
                while start < len(text):
                    end = min(start + self.chunk_size, len(text))
                    chunk_text = text[start:end]
                    
                    chunks.append({
                        'chunk_id': chunk_id,
                        'page_num': page_num,
                        'text': chunk_text,
                        'char_count': len(chunk_text),
                        'split_method': 'basic'
                    })
                    
                    chunk_id += 1
                    
                    if end >= len(text):
                        break
                        
                    start = end - self.chunk_overlap
        
        return chunks
    
    def split_into_chunks(self, pages_data: List[Dict]) -> List[Dict]:
        """ì²­í¬ ë¶„í•  ë¼ìš°í„°"""
        if self.use_langchain:
            return self.split_into_chunks_langchain(pages_data)
        else:
            return self.split_into_chunks_basic(pages_data)
    
    # ============================================
    # ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    # ============================================
    
    def process_pdf(self, pdf_filename: str) -> Optional[Dict]:
        """PDF íŒŒì¼ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        pdf_path = self.raw_folder / pdf_filename
        
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_filename}")
        print(f"{'='*60}")
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)
        print("\n[1ë‹¨ê³„] í…ìŠ¤íŠ¸ ì¶”ì¶œ")
        pages_data = self.extract_text_from_pdf(pdf_path)
        
        if not pages_data:
            print("  âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨!")
            return None
        
        # 2. ì²­í¬ ë¶„í• 
        print(f"\n[2ë‹¨ê³„] ì²­í¬ ë¶„í•  ({'LangChain' if self.use_langchain else 'ê¸°ë³¸'} ëª¨ë“œ)")
        chunks = self.split_into_chunks(pages_data)
        
        # 3. í†µê³„
        total_chars = sum(chunk['char_count'] for chunk in chunks if 'char_count' in chunk)
        non_empty_chunks = sum(1 for chunk in chunks if chunk.get('text'))
        avg_chunk_size = total_chars / non_empty_chunks if non_empty_chunks > 0 else 0
        
        print(f"  âœ“ ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        print(f"  âœ“ í…ìŠ¤íŠ¸ ìˆëŠ” ì²­í¬: {non_empty_chunks}ê°œ")
        print(f"  âœ“ ì´ ì¶”ì¶œ ë¬¸ì ìˆ˜: {total_chars:,} ì")
        print(f"  âœ“ í‰ê·  ì²­í¬ í¬ê¸°: {avg_chunk_size:.0f} ì")
        
        # 4. ê²°ê³¼ ì €ì¥
        output_data = {
            'source_file': pdf_filename,
            'total_pages': len(pages_data),
            'total_chunks': len(chunks),
            'total_characters': total_chars,
            'average_chunk_size': round(avg_chunk_size, 2),
            'chunks': chunks,
            'processing_info': {
                'chunk_size': self.chunk_size,
                'chunk_overlap': self.chunk_overlap,
                'ocr_dpi': self.ocr_dpi,
                'split_method': 'langchain' if self.use_langchain else 'basic',
                'methods_used': list(set(p['method'] for p in pages_data))
            }
        }
        
        # JSON ì €ì¥
        output_filename = pdf_path.stem + '_chunks.json'
        output_path = self.output_folder / output_filename
        
        print("\n[3ë‹¨ê³„] íŒŒì¼ ì €ì¥")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ“ ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return output_data
    
    def process_all_pdfs(self):
        """raw í´ë”ì˜ ëª¨ë“  PDF ì²˜ë¦¬"""
        pdf_files = list(self.raw_folder.glob("*.pdf"))
        
        if not pdf_files:
            print("\nâŒ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   ê²½ë¡œ í™•ì¸: {self.raw_folder.absolute()}")
            return []
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ PDF ì²˜ë¦¬ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ğŸ“ ëŒ€ìƒ í´ë”: {self.raw_folder.absolute()}")
        print(f"ğŸ“Š ë°œê²¬ëœ íŒŒì¼: {len(pdf_files)}ê°œ")
        print(f"âš™ï¸ ì²­í¬ ì„¤ì •: size={self.chunk_size}, overlap={self.chunk_overlap}")
        print(f"ğŸ”§ OCR DPI: {self.ocr_dpi}")
        print(f"{'='*60}")
        
        results = []
        success_count = 0
        
        for idx, pdf_file in enumerate(pdf_files, 1):
            print(f"\n[{idx}/{len(pdf_files)}]")
            try:
                result = self.process_pdf(pdf_file.name)
                if result:
                    results.append(result)
                    success_count += 1
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ìµœì¢… ìš”ì•½
        print(f"\n{'='*60}")
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ì„±ê³µ: {success_count}/{len(pdf_files)}ê°œ")
        print(f"  ì‹¤íŒ¨: {len(pdf_files) - success_count}ê°œ")
        
        if results:
            total_chunks = sum(r['total_chunks'] for r in results)
            total_chars = sum(r['total_characters'] for r in results)
            print(f"  ì´ ìƒì„±ëœ ì²­í¬: {total_chunks}ê°œ")
            print(f"  ì´ ì¶”ì¶œ ë¬¸ì: {total_chars:,}ì")
        
        print(f"  ì €ì¥ ìœ„ì¹˜: {self.output_folder.absolute()}")
        print(f"{'='*60}\n")
        
        return results


# ============================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ë°©ë²• 1: LangChain ì‚¬ìš© (ê¶Œì¥ - ìŠ¤ìº”ë³¸ ìµœì í™”)
    processor = HybridPDFProcessor(
        raw_folder="data/raw",
        output_folder="data/chunks",
        use_langchain=True,      # LangChain ì‚¬ìš©
        chunk_size=500,          # ìŠ¤ìº”ë³¸ì´ë¯€ë¡œ ì‘ê²Œ (ê¸°ì¡´ 1000 â†’ 500)
        chunk_overlap=50,        # ì˜¤ë²„ë© ì¤„ì„ (ê¸°ì¡´ 200 â†’ 50)
        ocr_dpi=300              # OCR í•´ìƒë„
    )
    
    # ë°©ë²• 2: ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
    # processor = HybridPDFProcessor(
    #     raw_folder="data/raw",
    #     output_folder="data/chunks",
    #     use_langchain=False,
    #     chunk_size=400,
    #     chunk_overlap=30,
    #     ocr_dpi=300
    # )
    
    # ëª¨ë“  PDF ì²˜ë¦¬
    processor.process_all_pdfs()


if __name__ == "__main__":
    main()